
This section outlines the boosting algorithms studied in this project, grouped into classical and modern frameworks. We focus on the core algorithmic principles, weak-learner settings, optimization strategies, and major differences that motivate the experimental comparisons in Section~\ref{sec:experiment}.

\subsection{Classical Boosting Methods}

\subsubsection{AdaBoost}
Adaptive Boosting (AdaBoost) is one of the earliest and most influential boosting algorithms. It constructs a strong classifier by iteratively reweighting observations to emphasize misclassified examples. At iteration $t$, AdaBoost fits a weak learner $h_t$ (typically a decision stump) to the weighted dataset and computes a coefficient
\[
\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \varepsilon_t}{\varepsilon_t}\right),
\]
where $\varepsilon_t$ is the weighted classification error. The final model is a weighted sum of weak learners,
\[
H(x) = \text{sign}\left( \sum_{t=1}^T \alpha_t\, h_t(x) \right).
\]
Its strengths include interpretability and simplicity, but it is known to be sensitive to label noise and less scalable to larger, high-dimensional datasets.

\subsubsection{Gradient Boosting Machine (GBM)}
The Gradient Boosting Machine (GBM) generalizes boosting to arbitrary differentiable loss functions. At each iteration, a regression tree is fit to the negative gradient (pseudo-residuals) of the loss with respect to model predictions. Subsequent trees are grown sequentially, forming an additive model:
\[
F_{t}(x) = F_{t-1}(x) + \eta\, h_t(x),
\]
where $\eta$ is a learning rate controlling shrinkage. Compared with AdaBoost, GBM provides greater flexibility through custom loss functions and deeper trees, but traditional GBM training is computationally slow and sensitive to hyperparameter choices.

\subsection{Modern Boosting Frameworks}

\subsubsection{XGBoost}
Extreme Gradient Boosting (XGBoost \cite{chen2016xgboost}) introduces system-level and algorithmic improvements to GBM, including:
\begin{itemize}
    \item second-order Taylor expansion of the loss for more precise tree construction,
    \item $L_1$ and $L_2$ regularization on leaf weights,
    \item parallelized tree building,
    \item block-based and cache-aware computations for speed,
    \item sparsity-aware split finding.
\end{itemize}
Its objective function incorporates a regularization term:
\[
\mathcal{L} = \sum_{i=1}^n \ell(y_i, \hat{y}_i) + \sum_{k=1}^K \left( \gamma T_k + \frac{1}{2} \lambda \sum_{j=1}^{T_k} w_{kj}^2 \right),
\]
where $T_k$ is the number of leaves in tree $k$ and $w_{kj}$ are leaf weights.

\subsubsection{LightGBM}
LightGBM is designed for efficiency on large datasets. Its key contributions include:
\begin{itemize}
    \item Gradient-based One-Side Sampling (GOSS), which retains samples with large gradients and randomly samples the rest;
    \item Exclusive Feature Bundling (EFB), which reduces dimensionality by grouping mutually exclusive features;
    \item leaf-wise tree growth with depth constraints, enabling deeper, more informative splits.
\end{itemize}
These innovations allow LightGBM to train significantly faster than classical GBM while maintaining or improving predictive accuracy.

\subsection{Summary of Algorithmic Differences}
Table~\ref{tab:method_summary} summarizes the major conceptual distinctions among the four boosting frameworks studied.

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Weak Learner} & \textbf{Optimization} & \textbf{Key Improvements} \\
\midrule
AdaBoost & decision stumps & weighted errors & adaptive reweighting \\
GBM & regression trees & gradient descent & flexible loss, shrinkage \\
XGBoost & regression trees & second-order gradients & regularization, parallelism \\
LightGBM & leaf-wise trees & histogram-based & GOSS, EFB, scalability \\
\bottomrule
\end{tabular}
\caption{Summary of key algorithmic features across boosting methods.}
\label{tab:method_summary}
\end{table}

\medskip

These methodological distinctions motivate the empirical study of predictive performance, computational efficiency, robustness, and scalability presented next.
