\documentclass{article}
\usepackage[legalpaper, margin=1in]{geometry}

\usepackage{amsmath,amsfonts,amssymb,graphicx,fancyhdr,latexsym,float,amsthm,bm,graphicx,mathtools,txfonts,enumitem,listings,verbatim,color}
\usepackage{epsfig}
\usepackage[ruled,vlined, linesnumbered]{algorithm2e}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    citecolor=blue,
    urlcolor=cyan,
}
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\bS}{\mathcal{S}}

\DeclareMathOperator{\tr}{tr}


\pagestyle{fancy}
\lhead{STOR 565 Final Project Paper}
\rhead{\textit{Team 13}}

\title{A Comparative Study of Classical and Modern Boosting Algorithms for Supervised Learning}
\author{Tony Luo,\; Will Kim,\; Sichen Li,\; Shang Peng,\; Jiaheng Guo\thanks{Please correspond to \texttt{jiaheng@unc.edu} for any questions regarding this paper and our code.}}
\date{} %Please don't insert a date

\begin{document}
\maketitle

\textit{\textcolor{red}{Hi team, I am using the template provided by Prof. Zhong. We can for now ignore the references in .bib file and focus on this single .tex file for synchronization.}
}

\section{Introduction}

% Boosting algorithms represent a powerful and widely adopted paradigm within supervised machine learning, particularly for tasks involving tabular data. Their ability to sequentially combine the predictions of multiple weak learners into a strong predictive model has led to implementations across numerous applications. Historically, methods such as \textbf{AdaBoost} laid the groundwork for ensemble learning, while the \textbf{Gradient Boosting Machine (GBM)} refined the approach by framing boosting as an additive model that iteratively minimizes a differentiable loss function. These classical algorithms established the fundamental principles upon which all subsequent boosting methods are built.

% In recent years, the field has seen the emergence of highly optimized implementations, most notably \textbf{XGBoost} and \textbf{LightGBM}. These modern frameworks have achieved widespread acclaim, frequently dominating data science competitions due to their exceptional speed, scalability, and predictive accuracy. They build upon the core concepts of gradient boosting while introducing significant architectural and algorithmic enhancements, such as advanced regularization, histogram-based split finding, and novel tree growth strategies.

% Despite the undisputed empirical success of these modern frameworks, a direct comparative analysis that systematically dissects \textit{why} they outperform their predecessors is often lacking. Choices of algorithms are often based on popular opinion rather than solid reasons. There remains a critical gap in quantifying the practical impact of these modern innovations---such as regularization, histogram-based optimization, and sampling strategies---under a unified experimental setup.

% This study aims to address this gap by conducting a comparative analysis of classical boosting algorithms, \textbf{AdaBoost} and \textbf{GBM}, alongside their modern counterparts, \textbf{XGBoost} and \textbf{LightGBM}. Our central research objective is to empirically evaluate and quantify the performance improvements offered by recent implementations and to identify the specific design innovations that underpin these enhancements through examining the following:

% \begin{itemize}
%     \item \textbf{Predictive performance:} measurable improvements over classical methods.
%     \item \textbf{Model complexity and overfitting:} effects of depth, shrinkage, and regularization.
%     \item \textbf{Computational efficiency:} impact of parallelization, histogram-based training, and sampling strategies.
%     \item \textbf{Feature importance stability:} consistency of learned feature rankings across folds and algorithms.
%     \item \textbf{Scalability:} relative performance on larger datasets, highlighting differences in system design.
% \end{itemize}

Our code is available at: \url{https://github.com/jiaheng-guo/stor565}

\section{Related Works}

Review of existing studies in related fields.

\section{Methodology}

\textit{\textcolor{red}{Please be aware that the paper structure Prof. Zhong provided in this template (Section 2 Related Works, Section 3 Methodology) is different from what we proposed in the group chat. Please move the mathematics explanations to Section 3, and reserve Section 2 for literature review, which covers the historical development of boosting algorithms, their applications, and comparisons in previous studies, with minimal mathematical details.}}

Describe the models, techniques, or algorithms you developed or compared. 

\newpage

\section{Experiments Results}
\label{sec:experiment}

This section describes the experimental setup used to compare AdaBoost, GBM, XGBoost, and LightGBM. All models were evaluated under a unified and reproducible framework with consistent preprocessing, hyperparameter tuning, and cross-validation procedures. Comparative analysis are then carried out, focusing on predictive performance, computational efficiency, and feature importance stability.

\subsection{Datasets and Preprocessing}

We evaluate the four boosting algorithms on several publicly available datasets from \textit{Kaggle} and \textit{UCI Machine Learning Repository} spanning 9 classification tasks and 4 regression tasks across various domains. Table \ref{tab:clf_ds} and \ref{tab:reg_ds} summary the characteristics of the datasets we used.
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{dataset} & \textbf{\#samples} & \textbf{\#features} & \textbf{\#classes} & \textbf{domain} & \textbf{type} & \textbf{link} \\
        \hline
        Adult Income & 48,842 & 14 & 2 & Social Science & Mixed & \href{https://archive.ics.uci.edu/ml/datasets/adult}{UCI} \\
        \hline
        Heart Disease & 303 & 13 & 2 & Health & Mixed & \href{https://archive.ics.uci.edu/dataset/45/heart+disease}{UCI} \\
        \hline
        Mushrooms & 8,124 & 22 & 2 & Biology & Categorical & \href{https://www.kaggle.com/datasets/uciml/mushroom-classification}{Kaggle} \\
        \hline
        Telco Customer Churn & 7,043 & 33 & 2 & Business & Mixed & \href{https://www.kaggle.com/datasets/yeanzc/telco-customer-churn-ibm-dataset}{Kaggle} \\
        \hline
        Breast Cancer & 569 & 30 & 2 & Health & Numerical & \href{https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic}{UCI} \\
        \hline
        Credit Card Fraud & 284,807 & 30 & 2 & Business & Numerical & \href{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}{Kaggle} \\
        \hline
        IMDB Movie Reviews & 49,582 & - & 2 & Entertainment & NLP & \href{https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews}{Kaggle} \\
        \hline
        MNIST & 70,000 & 784 & 10 & Computer Science & Image & \href{https://archive.ics.uci.edu/dataset/683/mnist+database+of+handwritten+digits}{UCI} \\
        \hline
        HIGGS & 11,000,000 & 28 & 2 & Physics & Numerical & \href{https://www.kaggle.com/datasets/ashishpatel26/higgs-boson-dataset}{Kaggle} \\
        \hline
    \end{tabular}
    \caption{Characteristics of the Classification Datasets Used}
    \label{tab:clf_ds}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{dataset} & \textbf{\#samples} & \textbf{\#features} & \textbf{domain} & \textbf{link} \\
        \hline
        California Housing & 20\,640 & 8 & Finance & \href{https://www.kaggle.com/datasets/camnugent/california-housing-prices}{Kaggle} \\
        \hline
        Ames House Prices & 1\,460 & 80 & Finance & \href{https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data}{Kaggle} \\
        \hline
        Wine Quality & 4\,898 & 11 & Commercial & \href{https://archive.ics.uci.edu/ml/datasets/wine+quality}{UCI} \\
        \hline
        Boston Housing & 506 & 13 & Finance & \href{https://archive.ics.uci.edu/ml/datasets/housing}{UCI} \\
        \hline
    \end{tabular}
    \caption{Characteristics of the Regression Datasets Used}
    \label{tab:reg_ds}
\end{table}

These classification and regression datasets vary in sample size, feature size, and subject domain, providing different scenarios for comprehensive evaluation of the boosting algorithms. The different data types involved, including numerical, categorical, images, texts, and time-series, also contribute to the diversity of the evaluation. Specifically, we included two datasets, IMDB Movie Reviews, which consists of text data of audience attitudes towards movies, and MNIST, the classical handwritten digits dataset, to examine the versatility of these algorithms on NLP and CV classification tasks. Moreover, the HIGGS dataset contains 11 million samples of particle collision data, making it a significant dataset for evaluating the performance of boosting algorithms on large-scale numerical data. The Credit Card Fraud dataset is highly imbalanced, with only 492 frauds out of 284,807 transactions. All datasets were first processed through the following shared pipeline and were subsequently adjusted as needed for each dataset:
\begin{enumerate}
    \item[(1)] Missing numerical features were imputed using medians; missing categorical features were imputed using the most frequent category.
    \item[(2)] Categorical variables were one-hot encoded for AdaBoost, GBM, and XGBoost, while LightGBM additionally took advantage of its built-in categorical support.
    \item[(3)] Numerical features were left unscaled, consistent with tree-based model characteristics.
    \item[(4)] Training and testing splits were generated using fixed random seeds with an 80/20 ratio.
\end{enumerate}

We further processed the NLP and CV datasets. For IMDB Movie Reviews, we utilized TF-IDF vectorization to convert text data into numerical features suitable for boosting algorithms. For MNIST, we flattened the 28x28 pixel images into 784-dimensional vectors. For MNIST, we adopted the following preprocessing strategies: we first flattened the 28x28 pixel images into 784-dimensional vectors, then we applied PCA to reduce dimensionality while retaining 95\% variance.

\subsection{Hyperparameter Tuning and Evaluation Metrics}

Hyperparameter optimization for each boosting algorithm was performed using \textit{Bayesian Optimization}, which adaptively explores the hyperparameter space by modeling the relationship between hyperparameters and validation performance. The search focused on the key parameters that most strongly influence boosting performance, including maximum tree depth, number of estimators, and learning rate. For classification tasks, logistic or exponential loss functions were used as appropriate, while regression models employed squared-error loss. To prevent information leakage and ensure unbiased estimates, we adopted a nested 5-fold cross-validation framework, where the inner loop performed Bayesian optimization and the outer loop produced final evaluation scores.

For classification problems, we evaluated model performance using accuracy, F1-score, and ROC--AUC, with the one-vs-rest formulation applied to multiclass settings. For regression tasks, we reported RMSE, MAE, and $R^2$. All evaluations were conducted using 5-fold cross-validation with fixed random seeds to ensure reproducibility, and deterministic algorithm settings were used when supported.

Additionally, we measured training time and inference latency across datasets of varying scales. Finally, we extracted feature-importance rankings from each model to examine the consistency and stability of feature attributions across folds and candidate algorithms.

\subsection{Comparative Analysis on Classification Tasks}

In this subsection, we present the comparative results of AdaBoost, GBM, XGBoost, and LightGBM on the classification datasets described in Section \ref{sec:experiment}. The AUC scores across different datasets and algorithms are summarized in Figure \ref{fig:auc}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9
    \textwidth]{../static/auc_legacy.png}
    \caption{AUC Comparison Across Boosting Algorithms}
    \label{fig:auc}
\end{figure}

\subsection{Comparative Analysis on Regression Tasks}

\section{Conclusion and Future Direction}

% Based on the comparison of classical and modern boosting algorithms, we systematically evaluate the impact of recent design innovations. Our key findings confirm that modern frameworks represent a significant advancement in both performance and efficiency. Across a suite of classification and regression tasks, \textbf{XGBoost} and \textbf{LightGBM} consistently achieved superior predictive accuracy compared to their predecessors. Specifically, histogram-based methods provided dramatic speedups in \textbf{computational efficiency}, while the built-in L1 and L2 regularization in XGBoost offered more robust control over \textbf{model complexity and overfitting}. Furthermore, our analysis of \textbf{feature importance stability} suggested that modern algorithms tend to produce more consistent feature rankings, thus enhancing their reliability.

% Aside from these clear results, there do exist several \textbf{limitations}. First, the analysis was confined to a selection of publicly available Kaggle datasets. While diverse, they may not capture the full spectrum of data characteristics in all real-world applications, such as  datasets with extremely high-cardinality categorical features or non-standard data distributions, to name a few. Second, while we employed a standardized hyperparameter tuning process, the vastness of the parameter search space means we cannot guarantee that the absolute optimal configuration for each algorithm was discovered. It is possible that classical models, in particular, could see further improvement with more advanced tuning.

% Based on these findings and limitations, several \textbf{future directions} could be deduced. A possible next step is to extend this comparative analysis to a wider and more diverse range of datasets, including those from different domains such as finance or bioinformatics, to further validate the \textbf{scalability} and generalizability of these algorithms. Potential techniques for hyperparameter tuning, such as Bayesian optimization, could be utilized more  to discover the peak performance of each model. Another valuable extension is to benchmark these boosting methods against other competing model classes, particularly deep learning architectures designed for tabular data, to better situate their performance within the broader machine learning landscape.

\cite{freund1997decision} % temporary citation to test bibliography, delete later

\bibliographystyle{abbrv}
\bibliography{main}

\newpage

\appendix

\section{Additional Experimental Results}

\end{document}
