\documentclass{article}
\usepackage[legalpaper, margin=1in]{geometry}

\usepackage{amsmath,amsfonts,amssymb,graphicx,fancyhdr,latexsym,float,amsthm,bm,graphicx,mathtools,txfonts,enumitem,listings,verbatim,color}
\usepackage{epsfig}
\usepackage[ruled,vlined, linesnumbered]{algorithm2e}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    citecolor=blue,
    urlcolor=cyan,
}
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\bS}{\mathcal{S}}

\DeclareMathOperator{\tr}{tr}


\pagestyle{fancy}
\lhead{STOR 565 Final Project Paper}
\rhead{\textit{Team 13}}

\title{A Comparative Study of Classical and Modern Boosting Algorithms for Supervised Learning}
\author{Tony Luo,\; Will Kim,\; Sichen Li,\; Shang Peng,\; Jiaheng Guo\thanks{Please correspond to \texttt{jiaheng@unc.edu} for any questions regarding this paper and our code.}}
\date{} %Please don't insert a date

\begin{document}
\maketitle

\section{Introduction}

Boosting algorithms represent a powerful and widely adopted paradigm within supervised machine learning, particularly for tasks involving tabular data. Their ability to sequentially combine the predictions of multiple weak learners into a strong predictive model has led to implementations across numerous applications. Historically, methods such as \textbf{AdaBoost} laid the groundwork for ensemble learning, while the \textbf{Gradient Boosting Machine (GBM)} refined the approach by framing boosting as an additive model that iteratively minimizes a differentiable loss function. These classical algorithms established the fundamental principles upon which all subsequent boosting methods are built.

In recent years, the field has seen the emergence of highly optimized implementations, most notably \textbf{XGBoost} and \textbf{LightGBM}. These modern frameworks have achieved widespread acclaim, frequently dominating data science competitions due to their exceptional speed, scalability, and predictive accuracy. They build upon the core concepts of gradient boosting while introducing significant architectural and algorithmic enhancements, such as advanced regularization, histogram-based split finding, and novel tree growth strategies.

Despite the undisputed empirical success of these modern frameworks, a direct comparative analysis that systematically dissects \textit{why} they outperform their predecessors is often lacking. Choices of algorithms are often based on popular opinion rather than solid reasons. There remains a critical gap in quantifying the practical impact of these modern innovations---such as regularization, histogram-based optimization, and sampling strategies---under a unified experimental setup.

This study aims to address this gap by conducting a comparative analysis of classical boosting algorithms, \textbf{AdaBoost} and \textbf{GBM}, alongside their modern counterparts, \textbf{XGBoost} and \textbf{LightGBM}. Our central research objective is to empirically evaluate and quantify the performance improvements offered by recent implementations and to identify the specific design innovations that underpin these enhancements through examining the following:

\begin{itemize}
    \item \textbf{Predictive performance:} measurable improvements over classical methods.
    \item \textbf{Model complexity and overfitting:} effects of depth, shrinkage, and regularization.
    \item \textbf{Computational efficiency:} impact of parallelization, histogram-based training, and sampling strategies.
    \item \textbf{Feature importance stability:} consistency of learned feature rankings across folds and algorithms.
    \item \textbf{Scalability:} relative performance on larger datasets, highlighting differences in system design.
\end{itemize}

\section{Related Works}

Review of existing studies in related fields.

\section{Methodology}

Now we describe the algorithms used in the LightGBM and XGBoost models.
First we introduce
the gradient boosting algorithm
following the presentation in Hastie et al.
in order to explain
the workings of the LightGBM and XGBoost algorithms.

The gradient boosting model is an additive model of trees
trained in a forward stagewise manner.
Let the training set consist of \(N\) instances, denoted
\(\{ x_1, \dots, x_N \} \), with \(x_i \in \R^p\)
where \(p\) is the number of features. 
We denote a tree as
\[
    T(x; \Theta ) = \sum_{j=1}^J I(x \in R_j)
\]
with parameters \(\Theta = \{ R_j, \gamma _j \}_{1}^J \),
where the \(R_j\) are regions that partition the space of
all joint predictor variable values so
that each \(x_i \in R_j\) is assigned by the tree to
the \(j\)-th leaf node, \(\gamma _j\) is
the output of the tree at the \(j\)-th leaf node,
and \(J\) is the number of leaves. Hence
the gradient boosting model is a sum of \(M\) trees
\[
    f_M(x) = \sum_{m=1}^M T(x ; \Theta _m)
\]
built iteratively, where for each iteration \(m\)
from \(m=1\) to \(m=M\) we solve
\[
    \hat{\Theta}_m = \arg \min_{\Theta _m} \sum_{i=1}^N
    L(y_i, f_{m-1} (x_i) + T(x_i; \Theta _m))
\]
for a choice of loss function \(L\).
For general loss criteria
we must approximate \(L\) in order to arrive at a simple and fast algorithm.
We do so by fitting \(T(x; \Theta _m)\) to the negative
gradient of \(L(\mathbf{f}) = \sum_{i=1}^N L(y_i, f(x_i))\) with regard to
\[
    \mathbf{f} = \begin{bmatrix}
        f(x_1) & f(x_2) & \cdots &  f(x_n) \\
    \end{bmatrix}^T
\]
evaluated at \(\mathbf{f} = \mathbf{f}_{m-1}\), in analogy to steepest descent (whereby we would
find the ideal \(\mathbf{f}\) by subtracting a multiple of the negative gradient from
\(\mathbf{f}\) and iterating with the new value).

Recall that we find approximate solutions for the optimal \(R_j\) by greedy
tree induction. In other words, we recursively partition the space of joint predictor
variable values by splitting on a feature \(j\) at a point \(s\)
and considering \[R_L(j, s) = \{ x_i | x_{ij} \leq s \}
\text{ and } R_R (j, s) = \{ x_i | x_{ij} > s \}\]
where the \(R_L(j,s)\) and \(R_R(j,s)\) are subsets of the points \(O\)
at the current node.
We choose each split by finding the feature \(j\) and
splitting point \(s\) that minimizes the squared error loss against the negative gradient
with the optimal \(\gamma_L\) and \(\gamma _R\) for each region:
\[
    \min_{j, s} [\min_{\gamma _L} \sum_{x_i \in R_L(j,s)} (g_i - \gamma_L)^2
    + \min _{\gamma _R} \sum_{x_i \in R_R(j, s)} (g_i - \gamma _R)^2]
\]
where \(g_i = \partial_{\mathbf{f} _i} L(y_i, \mathbf{f}_i) |_{\mathbf{f}_i = f_{m-1} (x_i)}\).
Note that the optimal \(\gamma \) for each inner minimization is given by the mean
of the \(g_i\) for \(x_i \in R(j,s)\):
\[
    \arg \min _{\gamma }\sum_{x_i \in R(j,s)} (g_i - \gamma)^2
    = \frac{1}{|R(j,s)|} \sum_{x_i \in R(j,s)} g_i.
\]
Minimizing the above objective in \(j,s\) is equivalent to maximizing the variance gain
at \(O\),
defined as
\[
    V_{O}(j,s) = \frac{1}{|O|} \left( \frac{(\sum_{x_i \in R_L(j,s)} g_i )^2}{|R_L(j,s)|}
    + \frac{(\sum_{x_i \in R_R(j,s)} g_i)^2}{|R_R(j,s)|}\right).
\]

LightGBM alters the traditional gradient boosting method by considering
only the data instances with the largest gradients, along with
a random sample of the remaining data instances
when splitting (\emph{Gradient-based One-Side Sampling},
or GOSS). This increases the efficiency of the method while still accurately
estimating the variance gain.

â€¦ (incomplete)


\section{Experiments Results}
\label{sec:experiment}

This section describes the experimental setup used to compare AdaBoost, GBM, XGBoost, and LightGBM. All models were evaluated under a unified and reproducible framework with consistent preprocessing, hyperparameter tuning with Optuna, and cross-validation procedures. Comparative analysis are then carried out, focusing on predictive performance, computational efficiency, and feature importance stability. Our code is available at: \url{https://github.com/jiaheng-guo/stor565}.

\subsection{Datasets and Preprocessing}

We evaluate the four boosting algorithms on several publicly available datasets from \textit{Kaggle} and \textit{UCI Machine Learning Repository} spanning 9 classification tasks and 4 regression tasks across various domains. Table \ref{tab:clf_ds} and \ref{tab:reg_ds} summary the characteristics of the datasets we used.
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{dataset} & \textbf{\#samples} & \textbf{\#features} & \textbf{\#classes} & \textbf{domain} & \textbf{type} & \textbf{link} \\
        \hline
        Adult Income & 48,842 & 14 & 2 & Social Science & Mixed & \href{https://archive.ics.uci.edu/ml/datasets/adult}{UCI} \\
        \hline
        Heart Disease & 303 & 13 & 2 & Health & Mixed & \href{https://archive.ics.uci.edu/dataset/45/heart+disease}{UCI} \\
        \hline
        Mushrooms & 8,124 & 22 & 2 & Biology & Categorical & \href{https://www.kaggle.com/datasets/uciml/mushroom-classification}{Kaggle} \\
        \hline
        Telco Customer Churn & 7,043 & 33 & 2 & Business & Mixed & \href{https://www.kaggle.com/datasets/yeanzc/telco-customer-churn-ibm-dataset}{Kaggle} \\
        \hline
        Breast Cancer & 569 & 30 & 2 & Health & Numerical & \href{https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic}{UCI} \\
        \hline
        Credit Card Fraud & 284,807 & 30 & 2 & Business & Numerical & \href{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}{Kaggle} \\
        \hline
        IMDB Movie Reviews & 49,582 & - & 2 & Entertainment & NLP & \href{https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews}{Kaggle} \\
        \hline
        MNIST & 70,000 & 784 & 10 & Computer Science & Image & \href{https://archive.ics.uci.edu/dataset/683/mnist+database+of+handwritten+digits}{UCI} \\
        \hline
        HIGGS & 11,000,000 & 28 & 2 & Physics & Numerical & \href{https://www.kaggle.com/datasets/ashishpatel26/higgs-boson-dataset}{Kaggle} \\
        \hline
    \end{tabular}
    \caption{Characteristics of the Classification Datasets Used}
    \label{tab:clf_ds}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{dataset} & \textbf{\#samples} & \textbf{\#features} & \textbf{domain} & \textbf{link} \\
        \hline
        California Housing & 20,640 & 8 & Real Estate & \href{https://www.kaggle.com/datasets/camnugent/california-housing-prices}{Kaggle} \\
        \hline
        Ames House Prices & 1,460 & 80 & Real Estate & \href{https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data}{Kaggle} \\
        \hline
        Wine Quality & 4,898 & 11 & Business & \href{https://archive.ics.uci.edu/ml/datasets/wine+quality}{UCI} \\
        \hline
        Superconductivity & 21,263 & 81 & Physics & \href{https://archive.ics.uci.edu/dataset/464/superconductivty+data}{UCI} \\
        \hline
    \end{tabular}
    \caption{Characteristics of the Regression Datasets Used}
    \label{tab:reg_ds}
\end{table}

These classification and regression datasets vary in sample size, feature size, and subject domain, providing different scenarios for comprehensive evaluation of the boosting algorithms. The different data types involved, including numerical, categorical, images, texts, and time-series, also contribute to the diversity of the evaluation. Specifically, we included two datasets, IMDB Movie Reviews, which consists of text data of audience attitudes towards movies, and MNIST, the classical handwritten digits dataset, to examine the versatility of these algorithms on NLP and CV classification tasks. Moreover, the HIGGS dataset contains 11 million samples of particle collision data, making it a significant dataset for evaluating the performance of boosting algorithms on large-scale numerical data. The Credit Card Fraud dataset is highly imbalanced, with only 492 frauds out of 284,807 transactions. All datasets were first processed through the following shared pipeline and were subsequently adjusted as needed for each dataset:
\begin{enumerate}
    \item[(1)] Missing numerical features were imputed using medians; missing categorical features were imputed using the most frequent category.
    \item[(2)] Categorical variables were one-hot encoded for AdaBoost, GBM, and XGBoost, while LightGBM additionally took advantage of its built-in categorical support.
    \item[(3)] Numerical features were left unscaled, consistent with tree-based model characteristics.
    \item[(4)] Training and testing splits were generated using fixed random seeds with an 80/20 ratio.
\end{enumerate}

We further processed the NLP and CV datasets. For IMDB Movie Reviews, we utilized TF-IDF vectorization to convert text data into numerical features suitable for boosting algorithms. For MNIST, we flattened the 28x28 pixel images into 784-dimensional vectors. For MNIST, we adopted the following preprocessing strategies: we first flattened the 28x28 pixel images into 784-dimensional vectors, then we applied PCA to reduce dimensionality while retaining 95\% variance.

\subsection{Hyperparameter Tuning and Evaluation Metrics}

Hyperparameter optimization for each boosting algorithm was performed using \textit{Bayesian Optimization}, which adaptively explores the hyperparameter space by modeling the relationship between hyperparameters and validation performance. The search focused on the key parameters that most strongly influence boosting performance, including maximum tree depth, number of estimators, and learning rate. For classification tasks, logistic or exponential loss functions were used as appropriate, while regression models employed squared-error loss. To prevent information leakage and ensure unbiased estimates, we adopted a nested 5-fold cross-validation framework, where the inner loop performed Bayesian optimization and the outer loop produced final evaluation scores.

For classification problems, we evaluated model performance using accuracy, F1-score, and ROC--AUC, with the one-vs-rest formulation applied to multiclass settings. For regression tasks, we reported RMSE, MAE, and $R^2$. All evaluations were conducted using 5-fold cross-validation with fixed random seeds to ensure reproducibility, and deterministic algorithm settings were used when supported.

Additionally, we measured training time and inference latency across datasets of varying scales. Finally, we extracted feature-importance rankings from each model to examine the consistency and stability of feature attributions across folds and candidate algorithms.

\subsection{Comparative Analysis on Classification Tasks}

In this subsection, we present the comparative results of AdaBoost, GBM, XGBoost, and LightGBM on the classification datasets described in Section \ref{sec:experiment}. The AUC scores across different datasets and algorithms are summarized in Figure \ref{fig:auc}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../static/auroc.png}
    \caption{AUC Comparison Across Boosting Algorithms}
    \label{fig:auc}
\end{figure}

From the results, we observe that modern boosting algorithms generally outperform their classical counterparts across most datasets, while there is no single algorithm that consistently dominates all others. Notably, XGBoost and LightGBM exhibit significant improvements in AUC scores on larger datasets such as HIGGS and Credit Card Fraud, likely due to their optimized handling of large-scale data through parallelization and histogram-based split finding. On smaller datasets like Heart Disease and Breast Cancer, the performance gap narrows, suggesting that the advantages of modern algorithms may be more pronounced in high-dimensional or large-sample scenarios. Results in terms of the other metrics (accuracy and F1-score) follow similar trends, due to space limitations, we only present AUC results here, please refer to Appendix \ref{sec:appendix_results} for complete results.

In terms of computational efficiency, LightGBM consistently demonstrates the fastest training times across datasets, attributed to its histogram-based approach and leaf-wise tree growth strategy. XGBoost also shows considerable speed improvements over GBM, particularly on larger datasets. AdaBoost, while competitive on smaller datasets, tends to lag in training speed as data size increases. The training time comparison is illustrated in Figure \ref{fig:clf_training_time}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{../static/clf_training_time.png}
    \caption{Training Time Comparison Across Boosting Algorithms \textcolor{red}{[To Be Updated]}}
    \label{fig:clf_training_time}
\end{figure}

\subsection{Comparative Analysis on Regression Tasks}

\section{Conclusion and Future Direction}

Based on the comparison of classical and modern boosting algorithms, we systematically evaluate the impact of recent design innovations. Our key findings confirm that modern frameworks represent a significant advancement in both performance and efficiency. Across a suite of classification and regression tasks, \textbf{XGBoost} and \textbf{LightGBM} consistently achieved superior predictive accuracy compared to their predecessors. Specifically, histogram-based methods provided dramatic speedups in \textbf{computational efficiency}, while the built-in L1 and L2 regularization in XGBoost offered more robust control over \textbf{model complexity and overfitting}. Furthermore, our analysis of \textbf{feature importance stability} suggested that modern algorithms tend to produce more consistent feature rankings, thus enhancing their reliability.

Aside from these clear results, there do exist several \textbf{limitations}. First, the analysis was confined to a selection of publicly available Kaggle datasets. While diverse, they may not capture the full spectrum of data characteristics in all real-world applications, such as  datasets with extremely high-cardinality categorical features or non-standard data distributions, to name a few. Second, while we employed a standardized hyperparameter tuning process, the vastness of the parameter search space means we cannot guarantee that the absolute optimal configuration for each algorithm was discovered. It is possible that classical models, in particular, could see further improvement with more advanced tuning.

Based on these findings and limitations, several \textbf{future directions} could be deduced. A possible next step is to extend this comparative analysis to a wider and more diverse range of datasets, including those from different domains such as finance or bioinformatics, to further validate the \textbf{scalability} and generalizability of these algorithms. Potential techniques for hyperparameter tuning, such as Bayesian optimization, could be utilized more  to discover the peak performance of each model. Another valuable extension is to benchmark these boosting methods against other competing model classes, particularly deep learning architectures designed for tabular data, to better situate their performance within the broader machine learning landscape.

\cite{freund1997decision} % temporary citation to test bibliography, delete later

\bibliographystyle{abbrv}
\bibliography{main}

\newpage

\appendix

\section{Additional Experimental Results}
\label{sec:appendix_results}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../static/accuracy.png}
    \caption{Accuracy Comparison Across Boosting Algorithms}
    \label{fig:accuracy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../static/f1_score.png}
    \caption{F1-score Comparison Across Boosting Algorithms}
    \label{fig:f1_score}
\end{figure}

\end{document}
