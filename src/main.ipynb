{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0f65db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/course/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from sklearn.base import clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_california_housing, fetch_openml, load_breast_cancer\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBClassifier = XGBRegressor = None\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"xgboost is not installed\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGBMClassifier = LGBMRegressor = None\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"lightgbm is not installed\")\n",
    "\n",
    "assert XGBOOST_AVAILABLE and LIGHTGBM_AVAILABLE, \"Please install both `xgboost` and `lightgbm` for complete reproducibility.\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8a764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured datasets: ['Adult Income', 'Heart Disease', 'Mushroom Classification', 'Telco Customer Churn', 'Breast Cancer', 'Credit Card Fraud', 'IMDB Movie Reviews']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:98: SyntaxWarning: \"\\?\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\?\"? A raw string is also an option.\n",
      "<>:98: SyntaxWarning: \"\\?\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\?\"? A raw string is also an option.\n",
      "/var/folders/cm/9lmn6ll511g1zw7h85lq56n40000gn/T/ipykernel_81518/1182588621.py:98: SyntaxWarning: \"\\?\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\?\"? A raw string is also an option.\n",
      "  \"question_count\": text.str.count(\"\\?\"),\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    key: str\n",
    "    pretty_name: str\n",
    "    loader: Callable[[], Tuple[pd.DataFrame, pd.Series]]\n",
    "    task: str = \"classification\"\n",
    "    notes: str = \"\"\n",
    "\n",
    "def _replace_question_marks(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    return frame.replace(\"?\", np.nan)\n",
    "\n",
    "def load_adult_income() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dataset = fetch_openml(name=\"adult\", version=2, as_frame=True)\n",
    "    X = _replace_question_marks(dataset.data.copy())\n",
    "    y = dataset.target.str.strip()\n",
    "    return X, y\n",
    "\n",
    "def load_heart_disease() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dataset = fetch_openml(name=\"heart-disease\", version=1, as_frame=True)\n",
    "    frame = dataset.frame.copy()\n",
    "    target_col = dataset.details.get(\"default_target_attribute\") if dataset.details else None\n",
    "    if not target_col:\n",
    "        target_candidates = [col for col in frame.columns if col.lower() in {\"target\", \"class\", \"label\"}]\n",
    "        target_col = target_candidates[0] if target_candidates else frame.columns[-1]\n",
    "    y = frame.pop(target_col)\n",
    "    X = _replace_question_marks(frame)\n",
    "    y = y.astype(str).str.lower()\n",
    "    return X, y\n",
    "\n",
    "def load_mushrooms() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dataset = fetch_openml(name=\"mushroom\", version=1, as_frame=True)\n",
    "    X = _replace_question_marks(dataset.data.copy())\n",
    "    y = dataset.target.astype(str).str.lower()\n",
    "    return X, y\n",
    "\n",
    "def load_telco_churn() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dataset = fetch_openml(data_id=42178, as_frame=True)\n",
    "    frame = dataset.frame.copy()\n",
    "    frame.columns = [col.strip() for col in frame.columns]\n",
    "    target = dataset.target\n",
    "    if target is not None and not target.empty:\n",
    "        y = target.astype(str).str.strip().str.lower()\n",
    "        target_name = target.name\n",
    "        if target_name in frame.columns:\n",
    "            frame = frame.drop(columns=[target_name])\n",
    "    else:\n",
    "        target_candidates = [col for col in frame.columns if col.lower() in {\"churn\", \"target\", \"class\", \"label\"}]\n",
    "        target_name = target_candidates[0] if target_candidates else frame.columns[-1]\n",
    "        y = frame.pop(target_name).astype(str).str.strip().str.lower()\n",
    "    if \"customerID\" in frame.columns:\n",
    "        frame = frame.drop(columns=[\"customerID\"])\n",
    "    if \"TotalCharges\" in frame.columns:\n",
    "        frame[\"TotalCharges\"] = pd.to_numeric(frame[\"TotalCharges\"].replace(\" \", np.nan), errors=\"coerce\")\n",
    "    X = _replace_question_marks(frame)\n",
    "    return X, y\n",
    "\n",
    "def load_breast_cancer_dataset() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dataset = load_breast_cancer(as_frame=True)\n",
    "    frame = dataset.frame.copy()\n",
    "    y = frame.pop(\"target\").map({0: \"malignant\", 1: \"benign\"})\n",
    "    return frame, y\n",
    "\n",
    "def load_credit_card_fraud(sample_size: int = 80000, random_state: int = 42) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dataset = fetch_openml(name=\"creditcard\", version=1, as_frame=True)\n",
    "    frame = dataset.frame.copy()\n",
    "    frame[\"Class\"] = frame[\"Class\"].astype(int)\n",
    "    if sample_size and sample_size < len(frame):\n",
    "        frac = sample_size / len(frame)\n",
    "        samples = []\n",
    "        for label, group in frame.groupby(\"Class\"):\n",
    "            n = max(1, int(round(len(group) * frac)))\n",
    "            samples.append(group.sample(n=min(len(group), n), random_state=random_state))\n",
    "        frame = pd.concat(samples).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    y = frame.pop(\"Class\").map({0: \"legitimate\", 1: \"fraud\"})\n",
    "    return frame, y\n",
    "\n",
    "def load_imdb_reviews(sample_size: int = 20000, random_state: int = 42) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    train_df = dataset[\"train\"].to_pandas()[[\"text\", \"label\"]]\n",
    "    test_df = dataset[\"test\"].to_pandas()[[\"text\", \"label\"]]\n",
    "    frame = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    if sample_size and sample_size < len(frame):\n",
    "        frame = frame.sample(n=sample_size, random_state=random_state)\n",
    "    text = frame[\"text\"].fillna(\"\")\n",
    "    letters_only = text.str.replace(r\"[^A-Za-z]+\", \" \", regex=True).str.strip()\n",
    "    word_counts = letters_only.str.split().str.len().fillna(0)\n",
    "    char_lengths = text.str.len()\n",
    "    avg_word_length = (letters_only.str.len() / word_counts.clip(lower=1)).fillna(0)\n",
    "    upper_ratio = (\n",
    "        text.str.replace(r\"[^A-Z]\", \"\", regex=True).str.len()\n",
    "        / char_lengths.replace(0, np.nan)\n",
    "    ).fillna(0)\n",
    "    features = pd.DataFrame({\n",
    "        \"char_length\": char_lengths,\n",
    "        \"word_count\": word_counts,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"exclamation_count\": text.str.count(\"!\"),\n",
    "        \"question_count\": text.str.count(\"\\?\"),\n",
    "        \"upper_ratio\": upper_ratio,\n",
    "    })\n",
    "    y = frame[\"label\"].map({0: \"negative\", 1: \"positive\"})\n",
    "    return features.reset_index(drop=True), y.reset_index(drop=True)\n",
    "\n",
    "DATASETS = [\n",
    "    DatasetConfig(key=\"adult\", pretty_name=\"Adult Income\", loader=load_adult_income, notes=\"Predict >50K annual income.\"),\n",
    "    DatasetConfig(key=\"heart\", pretty_name=\"Heart Disease\", loader=load_heart_disease, notes=\"Binary heart disease diagnosis.\"),\n",
    "    DatasetConfig(key=\"mushroom\", pretty_name=\"Mushroom Classification\", loader=load_mushrooms, notes=\"Edible vs poisonous.\"),\n",
    "    DatasetConfig(key=\"telco\", pretty_name=\"Telco Customer Churn\", loader=load_telco_churn, notes=\"Telecom churn prediction.\"),\n",
    "    DatasetConfig(key=\"breast_cancer\", pretty_name=\"Breast Cancer\", loader=load_breast_cancer_dataset, notes=\"Wisconsin diagnostic dataset.\"),\n",
    "    DatasetConfig(key=\"credit\", pretty_name=\"Credit Card Fraud\", loader=load_credit_card_fraud, notes=\"Highly imbalanced fraud detection.\"),\n",
    "    DatasetConfig(key=\"imdb\", pretty_name=\"IMDB Movie Reviews\", loader=load_imdb_reviews, notes=\"Sentiment features engineered from reviews.\"),\n",
    "]\n",
    "\n",
    "print(f\"Configured datasets: {[cfg.pretty_name for cfg in DATASETS]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c021224",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOSTING_ALGOS = [\"adaboost\", \"gbm\", \"xgboost\", \"lightgbm\"]\n",
    "\n",
    "def _build_model(task: str, algorithm: str, random_state: int):\n",
    "    algorithm = algorithm.lower()\n",
    "    if algorithm == \"adaboost\":\n",
    "        if task == \"classification\":\n",
    "            return AdaBoostClassifier(n_estimators=400, learning_rate=0.5, random_state=random_state)\n",
    "        return AdaBoostRegressor(n_estimators=500, learning_rate=0.3, loss=\"square\", random_state=random_state)\n",
    "    if algorithm == \"gbm\":\n",
    "        if task == \"classification\":\n",
    "            return GradientBoostingClassifier(random_state=random_state)\n",
    "        return GradientBoostingRegressor(random_state=random_state)\n",
    "    if algorithm == \"xgboost\":\n",
    "        if not XGBOOST_AVAILABLE:\n",
    "            raise ImportError(\"Install the `xgboost` package to run XGBoost models.\")\n",
    "        if task == \"classification\":\n",
    "            return XGBClassifier(\n",
    "                n_estimators=600,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                eval_metric=\"auc\",\n",
    "                tree_method=\"hist\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        return XGBRegressor(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"hist\",\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    if algorithm == \"lightgbm\":\n",
    "        if not LIGHTGBM_AVAILABLE:\n",
    "            raise ImportError(\"Install the `lightgbm` package to run LightGBM models.\")\n",
    "        if task == \"classification\":\n",
    "            return LGBMClassifier(\n",
    "                n_estimators=600,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                objective=\"binary\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        return LGBMRegressor(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            objective=\"regression_l2\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n",
    "\n",
    "def build_model_pipeline(\n",
    "    X: pd.DataFrame,\n",
    "    config: DatasetConfig,\n",
    "    task: str,\n",
    "    algorithm: str,\n",
    "    random_state: int = 42,\n",
    ") -> Pipeline:\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "    transformers = []\n",
    "    if numeric_features:\n",
    "        numeric_transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "        transformers.append((\"num\", numeric_transformer, numeric_features))\n",
    "\n",
    "    if categorical_features:\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "            ]\n",
    "        )\n",
    "        transformers.append((\"cat\", categorical_transformer, categorical_features))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "    model = _build_model(task, algorithm, random_state)\n",
    "    return Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", model)])\n",
    "\n",
    "def _scoring(task: str) -> Dict[str, str]:\n",
    "    if task == \"classification\":\n",
    "        return {\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "            \"f1_weighted\": \"f1_weighted\",\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "        }\n",
    "    return {\n",
    "        \"rmse\": \"neg_root_mean_squared_error\",\n",
    "        \"mae\": \"neg_mean_absolute_error\",\n",
    "        \"r2\": \"r2\",\n",
    "    }\n",
    "\n",
    "def _predict_proba(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(X)\n",
    "        if proba.ndim == 2 and proba.shape[1] > 1:\n",
    "            return proba[:, 1]\n",
    "        return proba.ravel()\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        decision = model.decision_function(X)\n",
    "        decision = np.asarray(decision)\n",
    "        if decision.ndim == 1:\n",
    "            decision = (decision - decision.min()) / (decision.max() - decision.min() + 1e-8)\n",
    "            return decision\n",
    "    return None\n",
    "\n",
    "def _compute_metrics(\n",
    "    task: str,\n",
    "    y_true: pd.Series,\n",
    "    y_pred: np.ndarray,\n",
    "    y_proba: np.ndarray | None = None,\n",
    "    encoder: LabelEncoder | None = None,\n",
    ") -> Tuple[Dict[str, float], str]:\n",
    "    if task == \"classification\":\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "            \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "        if y_proba is not None:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, y_proba)\n",
    "        else:\n",
    "            metrics[\"roc_auc\"] = np.nan\n",
    "        labels_true = encoder.inverse_transform(y_true) if encoder is not None else y_true\n",
    "        labels_pred = encoder.inverse_transform(y_pred) if encoder is not None else y_pred\n",
    "        report = classification_report(labels_true, labels_pred)\n",
    "    else:\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        metrics = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "        report = f\"RMSE: {rmse:.4f} MAE: {mae:.4f} R²: {r2:.4f}\"\n",
    "    return metrics, report\n",
    "\n",
    "def evaluate_dataset(\n",
    "    config: DatasetConfig,\n",
    "    algorithm: str,\n",
    "    test_size: float = 0.2,\n",
    "    cv: int = 5,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[Dict[str, float], str]:\n",
    "    X, y = config.loader()\n",
    "    if config.task == \"classification\":\n",
    "        if not isinstance(y, pd.Series):\n",
    "            y = pd.Series(y)\n",
    "        encoder = LabelEncoder()\n",
    "        y_encoded = pd.Series(\n",
    "            encoder.fit_transform(y),\n",
    "            index=y.index,\n",
    "            name=y.name or \"target\",\n",
    "        )\n",
    "    else:\n",
    "        encoder = None\n",
    "        y_encoded = y\n",
    "\n",
    "    stratify = y_encoded if config.task == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y_encoded,\n",
    "        test_size=test_size,\n",
    "        stratify=stratify,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    base_pipeline = build_model_pipeline(X, config, config.task, algorithm, random_state=random_state)\n",
    "    pipeline = clone(base_pipeline)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_proba = _predict_proba(pipeline, X_test)\n",
    "    test_metrics, detailed_report = _compute_metrics(config.task, y_test, y_pred, y_proba, encoder)\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        base_pipeline,\n",
    "        X,\n",
    "        y_encoded,\n",
    "        cv=cv,\n",
    "        scoring=_scoring(config.task),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    cv_summary = {}\n",
    "    for key, values in cv_results.items():\n",
    "        if not key.startswith(\"test_\"):\n",
    "            continue\n",
    "        metric = key.replace(\"test_\", \"\")\n",
    "        scores = np.asarray(values)\n",
    "        if config.task == \"regression\" and metric in {\"rmse\", \"mae\"}:\n",
    "            scores = -scores\n",
    "        cv_summary[f\"{metric}_mean\"] = scores.mean()\n",
    "        cv_summary[f\"{metric}_std\"] = scores.std()\n",
    "\n",
    "    result = {\n",
    "        \"dataset\": config.pretty_name,\n",
    "        \"task\": config.task,\n",
    "        \"model\": algorithm,\n",
    "        \"n_samples\": len(X),\n",
    "        \"n_features\": X.shape[1],\n",
    "        \"notes\": config.notes,\n",
    "    }\n",
    "    result.update({f\"test_{k}\": v for k, v in test_metrics.items()})\n",
    "    result.update({f\"cv_{k}\": v for k, v in cv_summary.items()})\n",
    "    return result, detailed_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Evaluating Adult Income | ADABOOST =====\n",
      "===== Evaluating Heart Disease | ADABOOST =====\n",
      "===== Evaluating Mushroom Classification | ADABOOST =====\n",
      "===== Evaluating Telco Customer Churn | ADABOOST =====\n",
      "===== Evaluating Breast Cancer | ADABOOST =====\n",
      "===== Evaluating Credit Card Fraud | ADABOOST =====\n",
      "===== Evaluating IMDB Movie Reviews | ADABOOST =====\n",
      "===== Evaluating Adult Income | GBM =====\n",
      "===== Evaluating Heart Disease | GBM =====\n",
      "===== Evaluating Mushroom Classification | GBM =====\n",
      "===== Evaluating Telco Customer Churn | GBM =====\n",
      "===== Evaluating Breast Cancer | GBM =====\n",
      "===== Evaluating Credit Card Fraud | GBM =====\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "reports: Dict[str, str] = {}\n",
    "\n",
    "for algorithm in BOOSTING_ALGOS:\n",
    "    if algorithm == \"xgboost\" and not XGBOOST_AVAILABLE:\n",
    "        print(\"Skipping XGBoost — package not installed.\")\n",
    "        continue\n",
    "    if algorithm == \"lightgbm\" and not LIGHTGBM_AVAILABLE:\n",
    "        print(\"Skipping LightGBM — package not installed.\")\n",
    "        continue\n",
    "    for cfg in DATASETS:\n",
    "        print(f\"===== Evaluating {cfg.pretty_name} | {algorithm.upper()} =====\")\n",
    "        result, report = evaluate_dataset(cfg, algorithm)\n",
    "        results.append(result)\n",
    "        reports[f\"{cfg.key}_{algorithm}\"] = report\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "auc_df = results_df[[\"dataset\", \"model\", \"test_roc_auc\"]].dropna()\n",
    "auc_df = auc_df.sort_values([\"dataset\", \"model\"])\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=auc_df, x=\"dataset\", y=\"test_roc_auc\", hue=\"model\", palette=\"viridis\")\n",
    "plt.ylabel(\"ROC-AUC (Test)\")\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.title(\"Boosting Models ROC-AUC Across Classification Datasets\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.legend(title=\"Model\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33c670",
   "metadata": {},
   "source": [
    "for algorithm in BOOSTING_ALGOS:\n",
    "    for cfg in DATASETS:\n",
    "        key = f\"{cfg.key}_{algorithm}\"\n",
    "        if key not in reports:\n",
    "            continue\n",
    "        print(\"\" + \"=\" * 80)\n",
    "        print(f\"{cfg.pretty_name.upper()} | {algorithm.upper()}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(reports[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ff915",
   "metadata": {},
   "source": [
    "## Modeling Utilities\n",
    "The functions below assemble a shared preprocessing + model pipeline:\n",
    "- numeric columns → median imputation + standard scaling\n",
    "- categorical columns → most-frequent imputation + one-hot encoding (ignore unseen levels)\n",
    "- model head → AdaBoost / GBM / XGBoost / LightGBM with tuned defaults per task\n",
    "\n",
    "Evaluation uses a stratified train/test split (classification) plus 5-fold cross-validation for stabler estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b512580",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOSTING_ALGOS = [\"adaboost\", \"gbm\", \"xgboost\", \"lightgbm\"]\n",
    "\n",
    "def _build_model(task: str, algorithm: str, random_state: int) -> object:\n",
    "    algorithm = algorithm.lower()\n",
    "    if algorithm == \"adaboost\":\n",
    "        if task == \"classification\":\n",
    "            return AdaBoostClassifier(n_estimators=400, learning_rate=0.5, random_state=random_state)\n",
    "        return AdaBoostRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.3,\n",
    "            loss=\"square\",\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    if algorithm == \"gbm\":\n",
    "        if task == \"classification\":\n",
    "            return GradientBoostingClassifier(random_state=random_state)\n",
    "        return GradientBoostingRegressor(random_state=random_state)\n",
    "    if algorithm == \"xgboost\":\n",
    "        if not XGBOOST_AVAILABLE:\n",
    "            raise ImportError(\"Install the `xgboost` package to run XGBoost models.\")\n",
    "        if task == \"classification\":\n",
    "            return XGBClassifier(\n",
    "                n_estimators=600,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=4,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                eval_metric=\"logloss\",\n",
    "                tree_method=\"hist\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        return XGBRegressor(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"hist\",\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    if algorithm == \"lightgbm\":\n",
    "        if not LIGHTGBM_AVAILABLE:\n",
    "            raise ImportError(\"Install the `lightgbm` package to run LightGBM models.\")\n",
    "        if task == \"classification\":\n",
    "            return LGBMClassifier(\n",
    "                n_estimators=600,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=-1,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                objective=\"binary\",\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        return LGBMRegressor(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=-1,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            objective=\"regression_l2\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n",
    "\n",
    "def build_model_pipeline(\n",
    "    X: pd.DataFrame,\n",
    "    task: str,\n",
    "    algorithm: str,\n",
    "    random_state: int = 42,\n",
    ") -> Pipeline:\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "    transformers = []\n",
    "    if numeric_features:\n",
    "        numeric_transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "        transformers.append((\"num\", numeric_transformer, numeric_features))\n",
    "\n",
    "    if categorical_features:\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "            ]\n",
    "        )\n",
    "        transformers.append((\"cat\", categorical_transformer, categorical_features))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers)\n",
    "    model = _build_model(task, algorithm, random_state)\n",
    "    return Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", model)])\n",
    "\n",
    "def _scoring(task: str) -> Dict[str, str]:\n",
    "    if task == \"classification\":\n",
    "        return {\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "            \"f1_weighted\": \"f1_weighted\",\n",
    "        }\n",
    "    return {\n",
    "        \"rmse\": \"neg_root_mean_squared_error\",\n",
    "        \"mae\": \"neg_mean_absolute_error\",\n",
    "        \"r2\": \"r2\",\n",
    "    }\n",
    "\n",
    "def _summarize_cv(cv_results: Dict[str, np.ndarray], task: str) -> Dict[str, float]:\n",
    "    summary = {}\n",
    "    for key, values in cv_results.items():\n",
    "        if not key.startswith(\"test_\"):\n",
    "            continue\n",
    "        metric = key.replace(\"test_\", \"\")\n",
    "        scores = np.asarray(values)\n",
    "        if task == \"regression\" and metric in {\"rmse\", \"mae\"}:\n",
    "            scores = -scores\n",
    "        summary[f\"{metric}_mean\"] = scores.mean()\n",
    "        summary[f\"{metric}_std\"] = scores.std()\n",
    "    return summary\n",
    "\n",
    "def _compute_metrics(task: str, y_true: pd.Series, y_pred: np.ndarray) -> Tuple[Dict[str, float], str]:\n",
    "    if task == \"classification\":\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "            \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "        report = classification_report(y_true, y_pred)\n",
    "    else:\n",
    "        rmse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        metrics = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "        report = f\"RMSE: {rmse:.4f} MAE: {mae:.4f} R²: {r2:.4f}\"\n",
    "    return metrics, report\n",
    "\n",
    "def evaluate_dataset(\n",
    "    config: DatasetConfig,\n",
    "    algorithm: str,\n",
    "    test_size: float = 0.2,\n",
    "    cv: int = 5,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[Dict[str, float], str]:\n",
    "    X, y = config.loader()\n",
    "    if config.task == \"classification\":\n",
    "        if not isinstance(y, pd.Series):\n",
    "            y = pd.Series(y)\n",
    "        y = pd.Series(pd.Categorical(y).codes, index=y.index, name=y.name or \"target\")\n",
    "    stratify = y if config.task == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        stratify=stratify,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    base_pipeline = build_model_pipeline(X, config.task, algorithm, random_state=random_state)\n",
    "    pipeline = clone(base_pipeline)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    test_metrics, detailed_report = _compute_metrics(config.task, y_test, y_pred)\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        base_pipeline,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=_scoring(config.task),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    cv_summary = _summarize_cv(cv_results, config.task)\n",
    "\n",
    "    result = {\n",
    "        \"dataset\": config.pretty_name,\n",
    "        \"task\": config.task,\n",
    "        \"model\": algorithm,\n",
    "        \"n_samples\": len(X),\n",
    "        \"n_features\": X.shape[1],\n",
    "        \"notes\": config.notes,\n",
    "    }\n",
    "    result.update({f\"test_{k}\": v for k, v in test_metrics.items()})\n",
    "    result.update({f\"cv_{k}\": v for k, v in cv_summary.items()})\n",
    "\n",
    "    return result, detailed_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa30beb",
   "metadata": {},
   "source": [
    "## Run Boosting Benchmark\n",
    "The next cells iterate through every dataset–algorithm pair, collect hold-out and cross-validation metrics, and dump human-readable reports. Expect XGBoost/LightGBM + House Prices to take the longest due to high-dimensional one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "reports: Dict[str, str] = {}\n",
    "\n",
    "for algorithm in BOOSTING_ALGOS:\n",
    "    if algorithm == \"xgboost\" and not XGBOOST_AVAILABLE:\n",
    "        print(\"Skipping XGBoost — package not installed.\")\n",
    "        continue\n",
    "    if algorithm == \"lightgbm\" and not LIGHTGBM_AVAILABLE:\n",
    "        print(\"Skipping LightGBM — package not installed.\")\n",
    "        continue\n",
    "    for cfg in DATASETS:\n",
    "        print(f\"===== Evaluating {cfg.pretty_name} | {algorithm.upper()} =====\")\n",
    "        result, report = evaluate_dataset(cfg, algorithm)\n",
    "        results.append(result)\n",
    "        reports[f\"{cfg.key}_{algorithm}\"] = report\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df = results_df[[\"dataset\", \"model\", \"test_roc_auc\"]].dropna()\n",
    "auc_df = auc_df.sort_values([\"dataset\", \"model\"])\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=auc_df, x=\"dataset\", y=\"test_roc_auc\", hue=\"model\", palette=\"viridis\")\n",
    "plt.ylabel(\"ROC-AUC (Test)\")\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.title(\"Boosting Models ROC-AUC Across Classification Datasets\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.legend(title=\"Model\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in BOOSTING_ALGOS:\n",
    "    for cfg in DATASETS:\n",
    "        key = f\"{cfg.key}_{algorithm}\"\n",
    "        if key not in reports:\n",
    "            continue\n",
    "        print(\"\" + \"=\" * 80)\n",
    "        print(f\"{cfg.pretty_name.upper()} | {algorithm.upper()}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(reports[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
